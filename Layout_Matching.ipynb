{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDGKF0p/kA0Ah9knOagCmH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kikunota/3d-reconstruction-prototype/blob/main/Layout_Matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "collapsed": true,
        "id": "srlWDRlE-udF",
        "outputId": "c08f9b9f-4d5a-47e8-f10a-d776f5941174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.8.1.78 which is incompatible.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.8.1.78 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.4 requires spacy<4, which is not installed.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.8.1.78 which is incompatible.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.8.1.78 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1617479290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 4) Sanity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numpy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# should be 1.26.x in default Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# should be 2.2.2 in default Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "🧰 Cell 1 — Setup (installs)\n",
        "\"\"\"\n",
        "\n",
        "# ✅ Clean Cell 1 for Colab (no TF/cuDF conflicts)\n",
        "# 1) Optional: remove unused libs that demand NumPy>=2 (quiet the warnings)\n",
        "!pip -q uninstall -y thinc spacy -y >/dev/null 2>&1 || true\n",
        "\n",
        "# 2) Pin OpenCV to a build compatible with NumPy 1.26 (what Colab ships)\n",
        "!pip -q install \"opencv-python-headless==4.8.1.78\"\n",
        "\n",
        "# 3) Install what we actually need\n",
        "!pip -q install \"faiss-cpu==1.8.0.post1\" \"open_clip_torch==2.26.1\"\n",
        "\n",
        "# 4) Sanity check\n",
        "import numpy, pandas as pd, faiss, torch, open_clip\n",
        "print(\"numpy:\", numpy.__version__)       # should be 1.26.x in default Colab\n",
        "print(\"pandas:\", pd.__version__)         # should be 2.2.2 in default Colab\n",
        "print(\"faiss:\", faiss.__version__)\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"open_clip_torch:\", open_clip.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell 2 — Imports & helpers\n",
        "\"\"\"\n",
        "\n",
        "import os, io, math, json, textwrap, random\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import faiss\n",
        "\n",
        "import open_clip  # CLIP family (OpenCLIP/SigLIP checkpoints)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "def cosine_sim_mat(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "    # a: [N, d], b: [M, d] -> [N, M]\n",
        "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
        "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-8)\n",
        "    return a_norm @ b_norm.T\n",
        "\n",
        "def show_hit(row, topk_df):\n",
        "    print(f\"\\nQUERY: {row.get('project','')} | {row.get('community','')} | size≈{row.get('size_min','')}–{row.get('size_max','')} m² | {row.get('features_text','')}\")\n",
        "    print(topk_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "tfTpyQIq-6f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "🧾 Cell 3 — Create a CSV template (download & fill locally)\n",
        "\n",
        "Use this once to get the template. Then re-upload with your data (and images).\n",
        "\"\"\"\n",
        "template = pd.DataFrame([\n",
        "    {\n",
        "        \"plan_filename\": \"TOWERA_08A.jpg\",  # ground-truth plan image filename (optional but needed for accuracy metrics)\n",
        "        \"project\": \"Downtown Tower A\",\n",
        "        \"community\": \"Downtown Dubai\",\n",
        "        \"tower\": \"A\",\n",
        "        \"size_min\": 80,          # in m² (or leave blank)\n",
        "        \"size_max\": 95,\n",
        "        \"level_band\": \"Floors 10-20\",  # free text\n",
        "        \"orientation\": \"Burj-facing\",  # free text\n",
        "        \"features_text\": \"2 bathrooms; closed kitchen; balcony; storage; laundry\"\n",
        "    },\n",
        "    {\n",
        "        \"plan_filename\": \"TOWERB_02B.jpg\",\n",
        "        \"project\": \"Marina Residences\",\n",
        "        \"community\": \"Dubai Marina\",\n",
        "        \"tower\": \"B\",\n",
        "        \"size_min\": 65,\n",
        "        \"size_max\": 72,\n",
        "        \"level_band\": \"Floors 2-8\",\n",
        "        \"orientation\": \"Partial marina view\",\n",
        "        \"features_text\": \"1 bathroom; open kitchen; balcony\"\n",
        "    }\n",
        "])\n",
        "\n",
        "# Save locally in Colab\n",
        "csv_path = \"property_profiles_template.csv\"\n",
        "template.to_csv(csv_path, index=False)\n",
        "\n",
        "# Trigger download to your computer\n",
        "files.download(csv_path)\n"
      ],
      "metadata": {
        "id": "oBWsEtkNC-2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "⬆️ Cell 4 — Upload your CSV & floor-plan images\n",
        "\n",
        "Prepare a folder of plan images (JPG/PNG/PDF first page as image), e.g. TOWERA_08A.jpg.\n",
        "\n",
        "CSV must have at least: project, community, features_text (others optional).\n",
        "\n",
        "If you add the correct plan filename under plan_filename, we’ll compute accuracy.\n",
        "\"\"\"\n",
        "from google.colab import files\n",
        "import zipfile, pathlib\n",
        "\n",
        "print(\"Upload your profiles CSV:\")\n",
        "uploaded_csv = files.upload()  # choose your file, e.g., profiles.csv\n",
        "csv_name = list(uploaded_csv.keys())[0]\n",
        "profiles = pd.read_csv(io.BytesIO(uploaded_csv[csv_name]))\n",
        "print(f\"Loaded {profiles.shape[0]} profiles\")\n",
        "\n",
        "# Upload a zip of images OR multiple individual images; both supported.\n",
        "print(\"Upload (a) a zip of floor-plan images OR (b) several image files:\")\n",
        "uploads = files.upload()\n",
        "\n",
        "IMG_DIR = \"/content/plans\"\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "for name, data in uploads.items():\n",
        "    if name.lower().endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(io.BytesIO(data), 'r') as zf:\n",
        "            zf.extractall(IMG_DIR)\n",
        "    else:\n",
        "        # write the file directly\n",
        "        out_path = os.path.join(IMG_DIR, name)\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            f.write(data)\n",
        "\n",
        "# Gather image paths (recursive)\n",
        "valid_ext = {\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".tif\",\".tiff\"}\n",
        "image_paths = []\n",
        "for root, _, files_ in os.walk(IMG_DIR):\n",
        "    for fn in files_:\n",
        "        if pathlib.Path(fn).suffix.lower() in valid_ext:\n",
        "            image_paths.append(os.path.join(root, fn))\n",
        "\n",
        "print(f\"Found {len(image_paths)} plan images.\")\n",
        "assert len(image_paths) > 0, \"No images found. Please upload plan images.\"\n"
      ],
      "metadata": {
        "id": "l7uizn1KDGnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "✍️ Cell 5 — Turn each profile into a CLIP text prompt\n",
        "\n",
        "Keep it short and factual—CLIP likes concise descriptions.\n",
        "\"\"\"\n",
        "\n",
        "def build_prompt(row: pd.Series) -> str:\n",
        "    parts = []\n",
        "    if pd.notna(row.get(\"project\")) and str(row[\"project\"]).strip():\n",
        "        parts.append(str(row[\"project\"]).strip())\n",
        "    if pd.notna(row.get(\"community\")) and str(row[\"community\"]).strip():\n",
        "        parts.append(str(row[\"community\"]).strip())\n",
        "    if pd.notna(row.get(\"tower\")) and str(row[\"tower\"]).strip():\n",
        "        parts.append(f\"Tower {str(row['tower']).strip()}\")\n",
        "    # Size band\n",
        "    smi, sma = row.get(\"size_min\"), row.get(\"size_max\")\n",
        "    if pd.notna(smi) and pd.notna(sma):\n",
        "        parts.append(f\"{int(smi)}–{int(sma)} m²\")\n",
        "    elif pd.notna(smi):\n",
        "        parts.append(f\"≥{int(smi)} m²\")\n",
        "    elif pd.notna(sma):\n",
        "        parts.append(f\"≤{int(sma)} m²\")\n",
        "    if pd.notna(row.get(\"level_band\")) and str(row[\"level_band\"]).strip():\n",
        "        parts.append(str(row[\"level_band\"]).strip())\n",
        "    if pd.notna(row.get(\"orientation\")) and str(row[\"orientation\"]).strip():\n",
        "        parts.append(str(row[\"orientation\"]).strip())\n",
        "    # Features (keep short)\n",
        "    feats = str(row.get(\"features_text\",\"\")).strip()\n",
        "    if feats:\n",
        "        parts.append(feats)\n",
        "    # Final prompt\n",
        "    return \", \".join(parts) + \". Floor plan layout.\"\n",
        "\n",
        "profiles[\"prompt\"] = profiles.apply(build_prompt, axis=1)\n",
        "profiles[[\"prompt\"]].head(5)\n"
      ],
      "metadata": {
        "id": "ptyERp6mIYIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "🖼️ Cell 6 — Load CLIP & preprocess\n",
        "\n",
        "Using a strong OpenCLIP checkpoint (EVA02-CLIP-B).\n",
        "\"\"\"\n",
        "# ✅ Cell 6 — Load CLIP model + preprocess (with safe fallbacks)\n",
        "import torch\n",
        "import open_clip\n",
        "from PIL import Image\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Prefer models that work well on line drawings / floor plans, then fall back.\n",
        "CANDIDATES = [\n",
        "    (\"EVA02-B-16\", \"merged2b_s8b_b131k\"),   # good on diagrams\n",
        "    (\"ViT-B-16\",   \"laion2b_s34b_b88k\"),    # widely available\n",
        "    (\"ViT-B-32\",   \"laion2b_s34b_b79k\"),    # most compatible fallback\n",
        "]\n",
        "\n",
        "model = preprocess = tokenizer = None\n",
        "last_error = None\n",
        "\n",
        "for MODEL_NAME, PRETRAINED in CANDIDATES:\n",
        "    try:\n",
        "        print(f\"Trying {MODEL_NAME} / {PRETRAINED} ...\")\n",
        "        _model, _, _preprocess = open_clip.create_model_and_transforms(\n",
        "            MODEL_NAME, pretrained=PRETRAINED, device=DEVICE\n",
        "        )\n",
        "        _tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
        "        _model.eval()\n",
        "        model, preprocess, tokenizer = _model, _preprocess, _tokenizer\n",
        "        print(f\"Loaded ✓ {MODEL_NAME} / {PRETRAINED} on {DEVICE}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        last_error = e\n",
        "        print(f\"→ Failed: {e}\")\n",
        "\n",
        "if model is None:\n",
        "    raise RuntimeError(f\"Could not load any CLIP checkpoint. Last error:\\n{last_error}\")\n",
        "\n",
        "print(\"Ready. DEVICE:\", DEVICE)\n"
      ],
      "metadata": {
        "id": "Ykqu_ze1Ie5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "⚙️ Cell 7 — Encode all images\n",
        "\n",
        "This precomputes image embeddings once; scale: thousands OK on Colab.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def load_image(path, max_side=2048):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    # (Optional) downscale huge images to speed up\n",
        "    w, h = img.size\n",
        "    scale = min(1.0, float(max_side)/max(w,h))\n",
        "    if scale < 1.0:\n",
        "        img = img.resize((int(w*scale), int(h*scale)), Image.BICUBIC)\n",
        "    return img\n",
        "\n",
        "batch = 64\n",
        "all_imgs, names = [], []\n",
        "for p in image_paths:\n",
        "    try:\n",
        "        img = load_image(p)\n",
        "        all_imgs.append(preprocess(img))\n",
        "        names.append(os.path.basename(p))\n",
        "    except Exception as e:\n",
        "        print(\"Bad image:\", p, e)\n",
        "\n",
        "img_t = torch.stack(all_imgs).to(DEVICE)\n",
        "img_embeds = []\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(img_t), batch), desc=\"Encoding images\"):\n",
        "        chunk = img_t[i:i+batch]\n",
        "        emb = model.encode_image(chunk)\n",
        "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "        img_embeds.append(emb)\n",
        "img_embeds = torch.cat(img_embeds, dim=0).float().cpu().numpy()\n",
        "\n",
        "print(\"Image embeddings:\", img_embeds.shape)\n"
      ],
      "metadata": {
        "id": "CcwLy-ZEJKGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "🔎 Cell 8 — Build FAISS index for fast top-K search\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "d = img_embeds.shape[1]\n",
        "index = faiss.IndexHNSWFlat(d, 32)   # simple, solid default for small/medium sets\n",
        "index.hnsw.efConstruction = 128\n",
        "index.hnsw.efSearch = 64\n",
        "index.add(img_embeds)\n",
        "print(\"FAISS index size:\", index.ntotal)\n"
      ],
      "metadata": {
        "id": "ltVFb52UJSCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "🧪 Cell 9 — Zero-shot retrieval (text → image) + metrics\n",
        "\n",
        "For each profile prompt, we encode text and search top-K plans.\n",
        "\n",
        "If your CSV includes plan_filename (exact match to an image filename), we compute Top-1/Top-5 accuracy.\n",
        "\"\"\"\n",
        "\n",
        "TOPK = 5\n",
        "\n",
        "def encode_texts(prompts: List[str]) -> np.ndarray:\n",
        "    out = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(prompts), 256):\n",
        "            tok = tokenizer(prompts[i:i+256]).to(DEVICE)\n",
        "            emb = model.encode_text(tok)\n",
        "            emb = emb / emb.norm(dim=-1, keepdim=True)\n",
        "            out.append(emb.float().cpu().numpy())\n",
        "    return np.vstack(out)\n",
        "\n",
        "text_vecs = encode_texts(profiles[\"prompt\"].tolist())\n",
        "D, I = index.search(text_vecs, TOPK)    # distances are inner products due to normalized embeddings\n",
        "\n",
        "# Assemble results\n",
        "results = []\n",
        "for r_idx, (dists, idxs) in enumerate(zip(D, I)):\n",
        "    top = [{\"rank\": j+1, \"filename\": names[i], \"score\": float(dists[j])} for j, i in enumerate(idxs)]\n",
        "    results.append(top)\n",
        "\n",
        "# Compute metrics if ground-truth available\n",
        "has_gt = \"plan_filename\" in profiles.columns and profiles[\"plan_filename\"].notna().any()\n",
        "\n",
        "top1, top5, cnt = 0, 0, 0\n",
        "rows_for_preview = []\n",
        "for i, top in enumerate(results):\n",
        "    gt = str(profiles.loc[i].get(\"plan_filename\",\"\")).strip()\n",
        "    if gt:\n",
        "        cnt += 1\n",
        "        top_files = [t[\"filename\"] for t in top]\n",
        "        if len(top_files)>0 and top_files[0] == gt: top1 += 1\n",
        "        if gt in top_files: top5 += 1\n",
        "    # store small preview table rows\n",
        "    for t in top:\n",
        "        if t[\"rank\"]<=3:  # keep short\n",
        "            rows_for_preview.append({\n",
        "                \"query_id\": i,\n",
        "                \"prompt\": profiles.loc[i, \"prompt\"][:120] + (\"...\" if len(profiles.loc[i, \"prompt\"])>120 else \"\"),\n",
        "                \"rank\": t[\"rank\"],\n",
        "                \"candidate\": t[\"filename\"],\n",
        "                \"score\": round(t[\"score\"], 3),\n",
        "                \"GT\": gt if gt else \"\"\n",
        "            })\n",
        "\n",
        "preview_df = pd.DataFrame(rows_for_preview)\n",
        "display(preview_df.head(20))\n",
        "\n",
        "if has_gt and cnt>0:\n",
        "    print(f\"\\nEval on {cnt} labeled queries:\")\n",
        "    print(f\"Top-1 accuracy: {top1/cnt:.3f}\")\n",
        "    print(f\"Top-5 accuracy: {top5/cnt:.3f}\")\n",
        "else:\n",
        "    print(\"\\nNo ground-truth 'plan_filename' provided; skipping accuracy.\")\n"
      ],
      "metadata": {
        "id": "8f8SJlyXJTH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "👀 Cell 10 — Inspect a few queries with their top-K\n",
        "\"\"\"\n",
        "sample_n = min(5, len(profiles))\n",
        "for i in range(sample_n):\n",
        "    topk_df = pd.DataFrame(results[i])[:TOPK]\n",
        "    show_hit(profiles.iloc[i], topk_df[[\"rank\",\"filename\",\"score\"]])\n"
      ],
      "metadata": {
        "id": "nZsp8a1VJlLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Cell 11 — Your own input → check for layout match (with optional GT check)\n",
        "import os, torch\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Prereqs: run Cell 4 (uploads), Cell 5 (build_prompt), Cell 6 (load model), Cell 7 (image encodings)\n",
        "assert 'model' in globals() and 'tokenizer' in globals() and 'img_embeds' in globals() and 'names' in globals(), \\\n",
        "    \"Please run Cells 4, 5, 6, and 7 first.\"\n",
        "name_to_path = {os.path.basename(p): p for p in image_paths}\n",
        "\n",
        "def build_prompt_from_fields(fields: dict) -> str:\n",
        "    \"\"\"Build a concise CLIP-friendly prompt from structured inputs (same keys as your CSV).\"\"\"\n",
        "    parts = []\n",
        "    for k in [\"project\", \"community\"]:\n",
        "        v = fields.get(k)\n",
        "        if v: parts.append(str(v).strip())\n",
        "    if fields.get(\"tower\"): parts.append(f\"Tower {str(fields['tower']).strip()}\")\n",
        "    smi, sma = fields.get(\"bed\"), fields.get(\"bath\")\n",
        "    if smi and sma: parts.append(f\"{int(smi)}–{int(sma)} m²\")\n",
        "    elif smi:       parts.append(f\"≥{int(smi)} m²\")\n",
        "    elif sma:       parts.append(f\"≤{int(sma)} m²\")\n",
        "    if fields.get(\"level_band\"): parts.append(str(fields[\"level_band\"]).strip())\n",
        "    if fields.get(\"orientation\"): parts.append(str(fields[\"orientation\"]).strip())\n",
        "    feats = str(fields.get(\"features_text\",\"\")).strip()\n",
        "    if feats: parts.append(feats)\n",
        "    return \", \".join(parts) + \". Floor plan layout.\"\n",
        "\n",
        "def match_layout(prompt: str = None,\n",
        "                 fields: dict = None,\n",
        "                 topk: int = 5,\n",
        "                 score_threshold: float = 0.30,\n",
        "                 expected: str = None,          # e.g., \"TOWERA_08A.jpg\" (ground-truth filename)\n",
        "                 show_images: int = 3):\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      - prompt, topk [(filename, score)], best (name, score), match (bool), threshold, expected_rank (or None)\n",
        "    \"\"\"\n",
        "    if (prompt is None) == (fields is None):\n",
        "        raise ValueError(\"Provide exactly one of: prompt OR fields.\")\n",
        "    if fields is not None:\n",
        "        prompt = build_prompt_from_fields(fields)\n",
        "\n",
        "    print(\"PROMPT:\", prompt)\n",
        "    with torch.no_grad():\n",
        "        tok  = tokenizer([prompt]).to(DEVICE)\n",
        "        tvec = model.encode_text(tok)\n",
        "        tvec = tvec / tvec.norm(dim=-1, keepdim=True)\n",
        "        sims = (tvec @ img_embeds.T).squeeze(0)         # [M]\n",
        "        scores, idxs = torch.topk(sims, k=min(topk, sims.shape[0]))\n",
        "\n",
        "    results = [(names[idxs[j].item()], float(scores[j].item())) for j in range(scores.shape[0])]\n",
        "\n",
        "    print(\"\\nTop matches:\")\n",
        "    for r, (fn, sc) in enumerate(results, start=1):\n",
        "        print(f\"{r:>2}. {fn:>30} | score={sc:.3f}\")\n",
        "\n",
        "    best_name, best_score = results[0]\n",
        "    is_match = best_score >= score_threshold\n",
        "    print(f\"\\nDecision (@ threshold {score_threshold:.2f}): {'MATCH ✅' if is_match else 'NO MATCH ❌'} \"\n",
        "          f\"(best={best_name}, score={best_score:.3f})\")\n",
        "\n",
        "    exp_rank = None\n",
        "    if expected:\n",
        "        try:\n",
        "            exp_rank = next((i+1 for i,(fn,_) in enumerate(results) if fn == expected), None)\n",
        "        except Exception:\n",
        "            exp_rank = None\n",
        "        if exp_rank:\n",
        "            print(f\"Ground truth '{expected}' found at rank {exp_rank} ✓\")\n",
        "        else:\n",
        "            print(f\"Ground truth '{expected}' NOT in top-{topk}.\")\n",
        "\n",
        "    if show_images:\n",
        "        print(\"\\nPreview:\")\n",
        "        for fn, _ in results[:show_images]:\n",
        "            path = name_to_path.get(fn)\n",
        "            if path:\n",
        "                img = Image.open(path).convert(\"RGB\")\n",
        "                w, h = img.size; max_w = 700\n",
        "                if w > max_w:\n",
        "                    img = img.resize((max_w, int(h*max_w/w)))\n",
        "                display(img)\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"topk\": results,\n",
        "        \"best\": (best_name, best_score),\n",
        "        \"match\": is_match,\n",
        "        \"threshold\": score_threshold,\n",
        "        \"expected_rank\": exp_rank\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# EXAMPLES — uncomment one and run:\n",
        "# 1) Free-text prompt\n",
        "\"\"\"out = match_layout(\n",
        "     prompt=\"Siena, Downtown Dubai, Tower A, 80–95 m², Floors 10-20, Burj-facing, \"\n",
        "            \"2 bathrooms; closed kitchen; balcony. Floor plan layout.\",\n",
        "     topk=5, score_threshold=0.30, expected=\"TOWERA_08A.jpg\"\n",
        ")\"\"\"\n",
        "\n",
        "#2) Structured fields (same keys as your CSV)\n",
        "out = match_layout(\n",
        "    fields={\n",
        "        \"project\": \"Siena\",\n",
        "        \"community\": \"Tuscan Residence\",\n",
        "        \"tower\": \"Siena 1\",\n",
        "        \"bed\": 1,\n",
        "        \"bath\": 2,\n",
        "        \"floorarea\": 908,\n",
        "        \"orientation\": \"unknown\",\n",
        "        \"features_text\": \"2 bathrooms; closed kitchen; balcony; storage; laundry\"\n",
        "    },\n",
        "    topk=5, score_threshold=0.30, expected=\"Siena.jpg\"\n",
        ")\n",
        "\n",
        "# Tip: Start with threshold 0.28–0.35 for EVA02/ViT-B-16; adjust after eyeballing a few scores.\n"
      ],
      "metadata": {
        "id": "EEZWYN9fUIFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}